{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets.mnist import MNIST\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "import os\n",
    "from ipywidgets import interact, FloatSlider\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from nn.enforced_ae import EnforcedAE\n",
    "from nn.ae import AE\n",
    "from train import train\n",
    "from schedule import cosine_schedule\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    }
   ],
   "source": [
    "train_dataset = MNIST(root='../../datasets', split='train', download=False, device=device, normalize=False, augment=True)\n",
    "val_dataset = MNIST(root='../../datasets', split='val', download=False, device=device, normalize=False)\n",
    "test_dataset = MNIST(root='../../datasets', split='test', download=False, device=device, normalize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 33\u001b[0m\n\u001b[1;32m     29\u001b[0m     os\u001b[38;5;241m.\u001b[39mmakedirs(save_dir)\n\u001b[1;32m     30\u001b[0m save_dir\u001b[38;5;241m=\u001b[39msave_dir \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrun_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrun_no\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pt\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 33\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimiser\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwriter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwriter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompute_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompute_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch_hyperparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhyperparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/RepEnforce/train.py:83\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_dataset, val_dataset, optimiser, num_epochs, batch_size, writer, compute_dtype, epoch_hyperparams, save_dir)\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m epoch_train_metrics:\n\u001b[1;32m     82\u001b[0m             epoch_train_metrics[key] \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 83\u001b[0m         epoch_train_metrics[key]\u001b[38;5;241m.\u001b[39mappend(\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mitem())\n\u001b[1;32m     85\u001b[0m \u001b[38;5;66;03m# ============================ VALIDATION ============================\u001b[39;00m\n\u001b[1;32m     87\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def train_model(model, experiment_name, trial_name):\n",
    "    optimiser = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "    num_epochs = 200\n",
    "    batch_size = 256\n",
    "    compute_dtype = torch.bfloat16\n",
    "\n",
    "    hyperparams = {\n",
    "        'lr': cosine_schedule(base=1e-3, end=1e-4, T=num_epochs, warmup=10, flat_end=10),\n",
    "        'wd': cosine_schedule(base=0.004, end=0.1, T=num_epochs)\n",
    "    }\n",
    "\n",
    "    trial_log_dir = f'out/logs/{experiment_name}/{trial_name}'\n",
    "    run_no = 0\n",
    "    while os.path.exists(trial_log_dir + f'/run_{run_no}'):\n",
    "        run_no += 1\n",
    "    writer = SummaryWriter(trial_log_dir + f'/run_{run_no}')\n",
    "    save_dir = f'out/models/{experiment_name}/{trial_name}/'\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    save_dir=save_dir + f'run_{run_no}.pt'\n",
    "\n",
    "\n",
    "    train(model, train_dataset, val_dataset, optimiser, num_epochs=num_epochs, batch_size=batch_size, writer=writer, compute_dtype=compute_dtype, save_dir=save_dir, epoch_hyperparams=hyperparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = 'base'\n",
    "trial_name = 'ae_cnn20_schedules'\n",
    "ae = AE(in_channels=1, z_dim=20, cnn=True).to(device)\n",
    "train_model(ae, experiment_name, trial_name)\n",
    "ae.load_state_dict(torch.load(f'out/models/{experiment_name}/{trial_name}/run_0.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = 'base'\n",
    "trial_name = 'enforced_ae_cnn20_schedules'\n",
    "enforced_ae = EnforcedAE(in_channels=1, z_dim=20, cnn=True).to(device)\n",
    "train_model(enforced_ae, experiment_name, trial_name)\n",
    "enforced_ae.load_state_dict(torch.load(f'out/models/{experiment_name}/{trial_name}/run_0.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enforced AE Learning\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "images, _= next(iter(test_loader))\n",
    "def plot_images_with_widgets(model, images):\n",
    "    def update(rotation=0., translate_x=0., translate_y=0., scale=1., shear=0.):\n",
    "        action = torch.tensor([rotation, translate_x, translate_y, scale, shear]).to(device)\n",
    "        images_aug, actions = model.transform_images(images, action)\n",
    "        images_hat = model(images, action)[0]\n",
    "\n",
    "        # Plot original images\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(12, 6))\n",
    "        axes[0].imshow(torchvision.utils.make_grid(images.cpu(), nrow=4, padding=2, normalize=True).permute(1, 2, 0))\n",
    "        axes[0].set_title('Original Inputs')\n",
    "        axes[0].axis('off')\n",
    "\n",
    "        axes[1].imshow(torchvision.utils.make_grid(images_aug.cpu(), nrow=4, padding=2, normalize=True).permute(1, 2, 0))\n",
    "        axes[1].set_title('Transformed Targets')\n",
    "        axes[1].axis('off')\n",
    "\n",
    "        axes[2].imshow(torchvision.utils.make_grid(images_hat.cpu(), nrow=4, padding=2, normalize=True).permute(1, 2, 0))\n",
    "        axes[2].set_title('Output Predictions')\n",
    "        axes[2].axis('off')\n",
    "\n",
    "        plt.show()\n",
    "    \n",
    "    interact(update, \n",
    "             rotation=FloatSlider(min=-1.0, max=1.0, step=0.1, value=0.),\n",
    "             translate_x=FloatSlider(min=-1.0, max=1.0, step=0.1, value=0.),\n",
    "             translate_y=FloatSlider(min=-1.0, max=1.0, step=0.1, value=0.),\n",
    "             scale=FloatSlider(min=-1.0, max=1.0, step=0.1, value=0.),\n",
    "             shear=FloatSlider(min=-1.0, max=1.0, step=0.1, value=0.))\n",
    "\n",
    "plot_images_with_widgets(enforced_ae, images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_with_latents(model, dataset, labelled=False):\n",
    "    idx = torch.randint(0, len(dataset), (1,))\n",
    "    image = dataset[idx][0]\n",
    "    z = model.infer(image)\n",
    "    def update(z0=0., z1=0., z2=0., z3=0., z4=0., z5=0., z6=0., z7=0., z8=0., z9=0.):\n",
    "        z[:, :10] = torch.tensor([z0, z1, z2, z3, z4, z5, z6, z7, z8, z9])\n",
    "        x_hat = model.decode(z).detach()\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "        axes[0].imshow(image.cpu().squeeze(0).permute(1, 2, 0), cmap='gray')\n",
    "        axes[0].set_title('Original Image')\n",
    "        axes[0].axis('off')\n",
    "\n",
    "        axes[1].imshow(x_hat.cpu().squeeze(0).permute(1, 2, 0), cmap='gray')\n",
    "        axes[1].set_title('Reconstructed Image')\n",
    "        axes[1].axis('off')\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "    sliders = {\n",
    "        \n",
    "        'z0': FloatSlider(min=-3.0, max=3.0, step=0.01, value=z[:,0].item(), description='(rotation) z0' if labelled else 'z0'),\n",
    "        'z1': FloatSlider(min=-3.0, max=3.0, step=0.01, value=z[:,1].item(), description='(x offset) z1' if labelled else 'z1'),\n",
    "        'z2': FloatSlider(min=-3.0, max=3.0, step=0.01, value=z[:,2].item(), description='(y offset) z2' if labelled else 'z2'),\n",
    "        'z3': FloatSlider(min=-3.0, max=3.0, step=0.01, value=z[:,3].item(), description='(scale) z3' if labelled else 'z3'),\n",
    "        'z4': FloatSlider(min=-3.0, max=3.0, step=0.01, value=z[:,4].item(), description='(shear) z4' if labelled else 'z4'),\n",
    "        'z5': FloatSlider(min=-3.0, max=3.0, step=0.01, value=z[:,5].item()),\n",
    "        'z6': FloatSlider(min=-3.0, max=3.0, step=0.01, value=z[:,6].item()),\n",
    "        'z7': FloatSlider(min=-3.0, max=3.0, step=0.01, value=z[:,7].item()),\n",
    "        'z8': FloatSlider(min=-3.0, max=3.0, step=0.01, value=z[:,8].item()),\n",
    "        'z9': FloatSlider(min=-3.0, max=3.0, step=0.01, value=z[:,9].item()),\n",
    "    }\n",
    "\n",
    "    interact(update, **sliders)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "play_with_latents(ae, test_dataset, labelled=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "play_with_latents(enforced_ae, test_dataset, labelled=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
